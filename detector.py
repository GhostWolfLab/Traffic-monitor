#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼‚å¸¸æ£€æµ‹æ¨¡å— - æ”¯æŒå¤šç§æ£€æµ‹ç®—æ³•
"""

import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from collections import defaultdict, deque
from datetime import datetime, timedelta
import pickle
import os

class TrafficDetector:
    """æµé‡å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self, method='isolation_forest'):
        self.method = method
        self.scaler = StandardScaler()
        self.models = {
            'isolation_forest': None,
            'autoencoder': None,
            'statistical': None
        }
        self.baseline = None
        self.traffic_history = defaultdict(lambda: deque(maxlen=100))
        
        # åŠ¨ç‰©æ¨¡å¼å®šä¹‰
        self.animal_patterns = {
            'ğŸ¦œ': {
                'name': 'å–‹å–‹ä¸ä¼‘çš„é¹¦é¹‰',
                'desc': 'é«˜é¢‘å°åŒ…é€šä¿¡',
                'condition': lambda f: f['packet_rate'] > 50 and f['avg_size'] < 200
            },
            'ğŸŠ': {
                'name': 'æ½œä¼çš„é³„é±¼',
                'desc': 'é•¿æ—¶é—´é™é»˜åçªå‘å¤§æµé‡',
                'condition': lambda f: f['burst_ratio'] > 5 and f['silence_duration'] > 300
            },
            'ğŸ¦ˆ': {
                'name': 'æ¸¸å¼‹çš„é²¨é±¼',
                'desc': 'ç«¯å£æ‰«æè¡Œä¸º',
                'condition': lambda f: f['unique_ports'] > 20 and f['avg_size'] < 100
            },
            'ğŸ˜': {
                'name': 'ç¬¨é‡çš„å¤§è±¡',
                'desc': 'å•æ¬¡å¤§æ•°æ®ä¼ è¾“',
                'condition': lambda f: f['avg_size'] > 5000 and f['packet_rate'] < 10
            },
            'ğŸ¦': {
                'name': 'å˜è‰²é¾™',
                'desc': 'åè®®é¢‘ç¹åˆ‡æ¢',
                'condition': lambda f: f['protocol_diversity'] > 0.7
            },
            'ğŸ': {
                'name': 'å¿™ç¢Œçš„èœœèœ‚',
                'desc': 'å¤šç›®æ ‡é€šä¿¡',
                'condition': lambda f: f['unique_dsts'] > 15
            },
            'ğŸ¦‡': {
                'name': 'å¤œè¡Œè™è ',
                'desc': 'éå¸¸è§„ç«¯å£é€šä¿¡',
                'condition': lambda f: f['uncommon_ports_ratio'] > 0.6
            },
            'ğŸ': {
                'name': 'ç›˜æ—‹çš„èŸ’è›‡',
                'desc': 'æŒç»­ç¨³å®šæµé‡',
                'condition': lambda f: f['packet_rate'] > 20 and f['std_size'] < 50
            },
            'ğŸ¦…': {
                'name': 'ä¿¯å†²çš„è€é¹°',
                'desc': 'SYNæ‰«æç‰¹å¾',
                'condition': lambda f: f['syn_ratio'] > 0.8 and f['packet_rate'] > 30
            },
            'ğŸ¢': {
                'name': 'ç¼“æ…¢çš„ä¹Œé¾Ÿ',
                'desc': 'æ…¢é€Ÿæ‰«æ',
                'condition': lambda f: f['packet_rate'] < 5 and f['unique_ports'] > 10
            }
        }
        
        self._init_models()
    
    def _init_models(self):
        """åˆå§‹åŒ–æ£€æµ‹æ¨¡å‹"""
        # Isolation Forest
        self.models['isolation_forest'] = IsolationForest(
            contamination=0.1,
            random_state=42,
            n_estimators=100
        )
        
        # Autoencoder (å»¶è¿Ÿåˆå§‹åŒ–)
        self.models['autoencoder'] = None
        self.autoencoder_trained = False
        
        # ç»Ÿè®¡åŸºçº¿æ¨¡å‹
        self.models['statistical'] = {
            'thresholds': {
                'packet_rate': {'mean': 0, 'std': 0},
                'avg_size': {'mean': 0, 'std': 0},
                'unique_dsts': {'mean': 0, 'std': 0},
                'protocol_diversity': {'mean': 0, 'std': 0}
            },
            'trained': False
        }
    
    def set_method(self, method):
        """è®¾ç½®æ£€æµ‹æ–¹æ³•"""
        if method in self.models:
            self.method = method
            return True
        return False
    
    def extract_features(self, packets):
        """ä»æ•°æ®åŒ…ä¸­æå–ç‰¹å¾"""
        features = []
        
        for i, packet in enumerate(packets):
            # è®¡ç®—æ—¶é—´çª—å£å†…çš„ç»Ÿè®¡ç‰¹å¾
            window_packets = packets[max(0, i-50):i+1]
            
            feature = self._compute_packet_features(packet, window_packets)
            features.append(feature)
        
        return features
    
    def _compute_packet_features(self, packet, window_packets):
        """è®¡ç®—å•ä¸ªæ•°æ®åŒ…çš„ç‰¹å¾"""
        # åŸºç¡€ç‰¹å¾
        feature_dict = {
            'length': packet.get('length', 0),
            'protocol': self._encode_protocol(packet.get('protocol', 'OTHER')),
            'src_port': packet.get('src_port', 0),
            'dst_port': packet.get('dst_port', 0),
        }
        
        # çª—å£ç»Ÿè®¡ç‰¹å¾
        if len(window_packets) > 0:
            sizes = [p.get('length', 0) for p in window_packets]
            dsts = [p.get('dst', '') for p in window_packets]
            protocols = [p.get('protocol', '') for p in window_packets]
            ports = [p.get('dst_port', 0) for p in window_packets]
            
            # è®¡ç®—æ—¶é—´ç‰¹å¾
            timestamps = [p.get('timestamp', 0) for p in window_packets]
            if len(timestamps) > 1:
                time_diffs = np.diff(timestamps)
                packet_rate = len(window_packets) / (max(timestamps) - min(timestamps) + 0.001)
            else:
                packet_rate = 0
                time_diffs = [0]
            
            # æµé‡ç»Ÿè®¡
            feature_dict.update({
                'packet_rate': packet_rate,
                'avg_size': np.mean(sizes),
                'std_size': np.std(sizes),
                'unique_dsts': len(set(dsts)),
                'unique_ports': len(set(ports)),
                'protocol_diversity': len(set(protocols)) / len(protocols) if protocols else 0,
                'uncommon_ports_ratio': sum(1 for p in ports if p > 10000) / len(ports) if ports else 0
            })
            
            # çªå‘ç‰¹å¾
            if len(time_diffs) > 0:
                avg_interval = np.mean(time_diffs)
                max_interval = np.max(time_diffs)
                feature_dict['burst_ratio'] = max_interval / (avg_interval + 0.001)
                feature_dict['silence_duration'] = max_interval
            else:
                feature_dict['burst_ratio'] = 0
                feature_dict['silence_duration'] = 0
            
            # SYNæ ‡å¿—ç»Ÿè®¡
            syn_count = sum(1 for p in window_packets if p.get('flags', {}).get('syn', False))
            feature_dict['syn_ratio'] = syn_count / len(window_packets) if window_packets else 0
        else:
            # é»˜è®¤å€¼
            feature_dict.update({
                'packet_rate': 0, 'avg_size': 0, 'std_size': 0,
                'unique_dsts': 0, 'unique_ports': 0, 'protocol_diversity': 0,
                'uncommon_ports_ratio': 0, 'burst_ratio': 0, 'silence_duration': 0,
                'syn_ratio': 0
            })
        
        return feature_dict
    
    def _encode_protocol(self, protocol):
        """ç¼–ç åè®®ç±»å‹"""
        protocol_map = {
            'TCP': 1, 'UDP': 2, 'ICMP': 3, 'HTTP': 4, 
            'HTTPS': 5, 'DNS': 6, 'SSH': 7, 'FTP': 8
        }
        return protocol_map.get(protocol.upper(), 0)
    
    def train(self, packets):
        """è®­ç»ƒæ¨¡å‹"""
        features = self.extract_features(packets)
        feature_vectors = self._dict_to_vector(features)
        
        if self.method == 'isolation_forest':
            self.models['isolation_forest'].fit(feature_vectors)
        
        elif self.method == 'statistical':
            # è®¡ç®—ç»Ÿè®¡åŸºçº¿
            for key in self.models['statistical']['thresholds']:
                values = [f[key] for f in features]
                self.models['statistical']['thresholds'][key] = {
                    'mean': np.mean(values),
                    'std': np.std(values)
                }
            self.models['statistical']['trained'] = True
        
        return True
    
    def _dict_to_vector(self, feature_dicts):
        """å°†ç‰¹å¾å­—å…¸è½¬æ¢ä¸ºå‘é‡"""
        keys = ['length', 'protocol', 'src_port', 'dst_port', 'packet_rate', 
                'avg_size', 'std_size', 'unique_dsts', 'unique_ports',
                'protocol_diversity', 'uncommon_ports_ratio', 'burst_ratio',
                'silence_duration', 'syn_ratio']
        
        vectors = []
        for fd in feature_dicts:
            vector = [fd.get(k, 0) for k in keys]
            vectors.append(vector)
        
        return np.array(vectors)
    
    def detect_anomalies(self, features):
        """æ£€æµ‹å¼‚å¸¸"""
        feature_vectors = self._dict_to_vector(features)
        
        if self.method == 'isolation_forest':
            predictions = self.models['isolation_forest'].predict(feature_vectors)
            scores = self.models['isolation_forest'].score_samples(feature_vectors)
            anomalies = predictions == -1
            
        elif self.method == 'autoencoder':
            return self._detect_autoencoder(feature_vectors, features)
            
        elif self.method == 'statistical':
            anomalies = []
            scores = []
            
            for feature in features:
                is_anomaly = False
                anomaly_score = 0
                
                for key, threshold in self.models['statistical']['thresholds'].items():
                    value = feature.get(key, 0)
                    mean = threshold['mean']
                    std = threshold['std']
                    
                    if std > 0:
                        z_score = abs(value - mean) / std
                        if z_score > 3:  # 3-sigmaè§„åˆ™
                            is_anomaly = True
                        anomaly_score = max(anomaly_score, z_score / 3)
                
                anomalies.append(is_anomaly)
                scores.append(-anomaly_score if is_anomaly else 0)
        
        else:
            anomalies = [False] * len(features)
            scores = [0] * len(features)
        
        return anomalies, scores
    
    def _detect_autoencoder(self, feature_vectors, features):
        """ä½¿ç”¨è‡ªç¼–ç å™¨æ£€æµ‹å¼‚å¸¸"""
        try:
            # å°è¯•å¯¼å…¥TensorFlow
            try:
                import tensorflow as tf
                from tensorflow import keras
                from tensorflow.keras import layers
                # è®¾ç½®æ—¥å¿—çº§åˆ«ï¼Œå‡å°‘è¾“å‡º
                tf.get_logger().setLevel('ERROR')
            except ImportError:
                print("âš ï¸ TensorFlowæœªå®‰è£…ï¼ŒAutoencoderæ–¹æ³•ä¸å¯ç”¨")
                print("   å®‰è£…å‘½ä»¤: pip install tensorflow")
                print("   é™çº§ä½¿ç”¨ Isolation Forest")
                # é™çº§åˆ°Isolation Forest
                predictions = self.models['isolation_forest'].predict(feature_vectors)
                scores = self.models['isolation_forest'].score_samples(feature_vectors)
                return predictions == -1, scores
            
            # å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œåˆ›å»ºå¹¶è®­ç»ƒ
            if not self.autoencoder_trained or self.models['autoencoder'] is None:
                print("ğŸ§  åˆå§‹åŒ– Autoencoder æ¨¡å‹...")
                
                # æ£€æŸ¥æ•°æ®é‡
                if len(feature_vectors) < 50:
                    print(f"âš ï¸ æ•°æ®é‡ä¸è¶³({len(feature_vectors)}ä¸ª)ï¼Œè‡³å°‘éœ€è¦50ä¸ªæ ·æœ¬")
                    print("   é™çº§ä½¿ç”¨ Isolation Forest")
                    predictions = self.models['isolation_forest'].predict(feature_vectors)
                    scores = self.models['isolation_forest'].score_samples(feature_vectors)
                    return predictions == -1, scores
                
                # æ ‡å‡†åŒ–ç‰¹å¾
                normalized_features = self.scaler.fit_transform(feature_vectors)
                
                # å®šä¹‰è‡ªç¼–ç å™¨ç»“æ„
                input_dim = normalized_features.shape[1]
                encoding_dim = max(4, input_dim // 2)
                
                # ç¼–ç å™¨
                input_layer = keras.Input(shape=(input_dim,))
                encoded = layers.Dense(encoding_dim * 2, activation='relu')(input_layer)
                encoded = layers.Dropout(0.2)(encoded)
                encoded = layers.Dense(encoding_dim, activation='relu')(encoded)
                
                # è§£ç å™¨
                decoded = layers.Dense(encoding_dim * 2, activation='relu')(encoded)
                decoded = layers.Dropout(0.2)(decoded)
                decoded = layers.Dense(input_dim, activation='linear')(decoded)
                
                # å®Œæ•´æ¨¡å‹
                autoencoder = keras.Model(input_layer, decoded)
                autoencoder.compile(
                    optimizer=keras.optimizers.Adam(learning_rate=0.001),
                    loss='mse'
                )
                
                # è®­ç»ƒæ¨¡å‹
                print(f"ğŸ“š è®­ç»ƒ Autoencoder (æ ·æœ¬æ•°: {len(normalized_features)})...")
                history = autoencoder.fit(
                    normalized_features, normalized_features,
                    epochs=50,
                    batch_size=min(32, len(normalized_features) // 2),
                    shuffle=True,
                    verbose=0,
                    validation_split=0.1
                )
                
                self.models['autoencoder'] = autoencoder
                self.autoencoder_trained = True
                final_loss = history.history['loss'][-1]
                print(f"âœ… Autoencoder è®­ç»ƒå®Œæˆ (æœ€ç»ˆloss: {final_loss:.4f})")
            
            # æ ‡å‡†åŒ–å½“å‰ç‰¹å¾
            normalized_features = self.scaler.transform(feature_vectors)
            
            # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
            reconstructed = self.models['autoencoder'].predict(normalized_features, verbose=0)
            
            # è®¡ç®—é‡æ„è¯¯å·®ï¼ˆMSEï¼‰
            mse = np.mean(np.power(normalized_features - reconstructed, 2), axis=1)
            
            # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼ï¼ˆåŸºäºé‡æ„è¯¯å·®çš„åˆ†ä½æ•°ï¼‰
            threshold = np.percentile(mse, 90)  # 90åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
            
            # æ ‡è®°å¼‚å¸¸ï¼ˆé‡æ„è¯¯å·®å¤§äºé˜ˆå€¼ï¼‰
            anomalies = mse > threshold
            
            # å½’ä¸€åŒ–åˆ†æ•°åˆ°[-1, 1]èŒƒå›´
            # é‡æ„è¯¯å·®è¶Šå¤§ï¼Œå¼‚å¸¸åˆ†æ•°è¶Šä½ï¼ˆè¶Šè´Ÿï¼‰
            max_mse = np.max(mse)
            if max_mse > 0:
                # å°†MSEæ˜ å°„åˆ°[-1, 0]åŒºé—´ï¼Œå¼‚å¸¸çš„åˆ†æ•°æ›´è´Ÿ
                scores = -1 * (mse / max_mse)
            else:
                scores = np.zeros(len(mse))
            
            return anomalies, scores
            
        except Exception as e:
            print(f"âŒ Autoencoder æ£€æµ‹å¤±è´¥: {e}")
            print("   é™çº§ä½¿ç”¨ Isolation Forest")
            predictions = self.models['isolation_forest'].predict(feature_vectors)
            scores = self.models['isolation_forest'].score_samples(feature_vectors)
            return predictions == -1, scores
    
    def classify_pattern(self, feature):
        """åˆ†ç±»æµé‡æ¨¡å¼ï¼ˆåŠ¨ç‰©ä»£å·ï¼‰"""
        for emoji, pattern in self.animal_patterns.items():
            try:
                if pattern['condition'](feature):
                    return {
                        'emoji': emoji,
                        'name': pattern['name'],
                        'description': pattern['desc']
                    }
            except:
                continue
        
        return {
            'emoji': 'ğŸ±',
            'name': 'æ™®é€šæµé‡',
            'description': 'æ­£å¸¸é€šä¿¡æ¨¡å¼'
        }
    
    def analyze_packets(self, packets):
        """åˆ†ææ•°æ®åŒ…åˆ—è¡¨"""
        if len(packets) < 10:
            return {
                'anomalies': [],
                'patterns': {}
            }
        
        # æå–ç‰¹å¾
        features = self.extract_features(packets)
        
        # è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not self.models['statistical']['trained']:
            self.train(packets)
        
        # æ£€æµ‹å¼‚å¸¸
        anomalies, scores = self.detect_anomalies(features)
        
        # åˆ†ç±»æ¨¡å¼
        patterns = defaultdict(list)
        results = []
        
        for i, (packet, feature, is_anomaly, score) in enumerate(zip(packets, features, anomalies, scores)):
            pattern = self.classify_pattern(feature)
            
            result = {
                'index': i,
                'timestamp': packet.get('timestamp', 0),
                'src': packet.get('src', ''),
                'dst': packet.get('dst', ''),
                'protocol': packet.get('protocol', ''),
                'length': packet.get('length', 0),
                'is_anomaly': bool(is_anomaly),
                'score': float(score),
                'pattern': pattern,
                'payload': packet.get('payload', '')
            }
            
            results.append(result)
            
            if is_anomaly:
                patterns[pattern['emoji']].append(result)
        
        return {
            'anomalies': [r for r in results if r['is_anomaly']],
            'patterns': dict(patterns),
            'all_results': results
        }
    
    def analyze_single_packet(self, packet):
        """åˆ†æå•ä¸ªæ•°æ®åŒ…ï¼ˆå®æ—¶ç›‘å¬ï¼‰"""
        # å°†æ•°æ®åŒ…æ·»åŠ åˆ°å†å²è®°å½•
        src = packet.get('src', 'unknown')
        self.traffic_history[src].append(packet)
        
        # è·å–çª—å£æ•°æ®
        window_packets = list(self.traffic_history[src])
        
        # è®¡ç®—ç‰¹å¾
        feature = self._compute_packet_features(packet, window_packets)
        
        # æ£€æµ‹å¼‚å¸¸
        if self.models['statistical']['trained']:
            feature_vector = self._dict_to_vector([feature])
            
            if self.method == 'isolation_forest':
                prediction = self.models['isolation_forest'].predict(feature_vector)[0]
                score = self.models['isolation_forest'].score_samples(feature_vector)[0]
                is_anomaly = prediction == -1
            else:
                is_anomaly = False
                score = 0
                for key, threshold in self.models['statistical']['thresholds'].items():
                    value = feature.get(key, 0)
                    mean = threshold['mean']
                    std = threshold['std']
                    if std > 0:
                        z_score = abs(value - mean) / std
                        if z_score > 3:
                            is_anomaly = True
                        score = max(score, z_score / 3)
                score = -score if is_anomaly else 0
        else:
            is_anomaly = False
            score = 0
        
        # åˆ†ç±»æ¨¡å¼
        pattern = self.classify_pattern(feature)
        
        return {
            'is_anomaly': is_anomaly,
            'score': float(score),
            'pattern': pattern,
            'feature': feature
        }
